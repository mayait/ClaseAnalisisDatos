{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayait/ClaseAnalisisDatos/blob/main/nlp/NLP_Tokenization_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Nombre del estudiante\n",
        "Estudiante = \"\" #@param {type:\"string\"}\n",
        "C√≥digo = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rXshJM2YLQvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdYh71TAa1Is"
      },
      "source": [
        "# NLP Procesamiento Natural de Lenguaje\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaFZyxpAa1Iw"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Empecemos desde donde quedamos en el ultimo ejercicio, importemos el archivo de Excel que exportaste con un listado de Tweets.\n",
        "* Asegurate que la columna con los tweets se llama **Tweet_Text**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkSpye9ykwTT"
      },
      "outputs": [],
      "source": [
        "#!pip install wordcloud\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH1dgS8oEquV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('Spiderman.xlsx')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIX5Y7ga1I7"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JceB30m2a1I8"
      },
      "source": [
        "**Pasemos todo a minusculas**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFpfMtZLa1I8"
      },
      "outputs": [],
      "source": [
        "df['Tweet_Text'] = df['Tweet_Text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZhnZ9l6FICV"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDYI0ltFa1I9"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbWS7waXnTPN"
      },
      "source": [
        "# **Tokenizar**\n",
        "En NLP el proceso de convertir las palabras o p√°rrafos en inputs para la computadora se llama tokenizaci√≥n. Se puede pensar al token como la unidad para procesamiento sem√°ntico, es decir palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2orNbE7NWPM"
      },
      "source": [
        "## Tokenizaci√≥n con NLTK\n",
        "El kit de herramientas de lenguaje natural, o m√°s com√∫nmente NLTK, es un conjunto de bibliotecas y programas para el procesamiento del lenguaje natural (PLN) simb√≥lico y estad√≠sticos para el lenguaje de programaci√≥n Python. NLTK incluye demostraciones gr√°ficas y datos de muestra.\n",
        "\n",
        "*Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O‚ÄôReilly Media Inc.*\n",
        "[nltk.org](https://nltk.org/)\n",
        "\n",
        "**NLTK tiene librerias que permiten tokenizar, veamos la diferencia entre ellas**\n",
        "\n",
        "TreeBankTokenizer es perfecto en partir el texto, sin embargo no es bueno con Tweets, quita informaci√≥n como hashtags y menciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AIb8JlEU5Jaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx0iH7YsO7TE"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer \n",
        "from nltk.tokenize import TreebankWordTokenizer \n",
        "\n",
        "texto = \"¬øCu√°nto tiempo pas√≥ desde que com√≠ una manzana?\"\n",
        "tweet = ' #Oscar a Mejor Pel√≠cula Internacional; üò©üòîüòÅ un filme ecuatoriano figura en la lista ¬ª https://t.co/cb5p6eX3DZ https://t.co/kQ0nVAbyOn'\n",
        "\n",
        "print(WordPunctTokenizer().tokenize(texto))\n",
        "print(WordPunctTokenizer().tokenize(tweet))\n",
        "print(TreebankWordTokenizer().tokenize(tweet))\n",
        "print(WordPunctTokenizer().tokenize(tweet))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9K3Or0WOluE"
      },
      "source": [
        "## NLTK Tweet Tokenizer\n",
        "\n",
        "Por suerte NLTK ya tiene una libreria que soluciona este problema y tokeniza manteniendo emoticonos, menciones y hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17hMdDIIRHoH"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "tweet = ' #Oscar a Mejor Pel√≠cula Internacional; üò©üòîüòÅ un filme ecuatoriano figura en la lista ¬ª https://t.co/cb5p6eX3DZ https://t.co/kQ0nVAbyOn'\n",
        "tweet_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMJJCZxwnyIt"
      },
      "source": [
        "# Creamos una lista de palabras y sus frecuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfJoSH1kl4ay"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "# Una lista para todas las palabras\n",
        "palabras_todas = []\n",
        "\n",
        "# Creamos un contador de frecuencias\n",
        "count_all = Counter()\n",
        "\n",
        "for tweet in df[\"Tweet_Text\"]:\n",
        "  # Crear una lista de todos los terminos con NLTK\n",
        "  terms_all = tweet_tokenizer.tokenize(tweet)\n",
        "  # Actualizar el contador\n",
        "  count_all.update(terms_all)\n",
        "  # Aumentar al listado de todas las palabras\n",
        "  palabras_todas.extend(terms_all)\n",
        "\n",
        "# Las primeras 10 palabras m√°s comunes\n",
        "print(\"Cantidad de terminos unicos: \", len(count_all))\n",
        "print(\"Total de terminos (incluye repetidos): \", len(palabras_todas))\n",
        "count_all.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0zsXIS9bnk2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "unique_string=(\" \").join(palabras_todas)\n",
        "wordcloud = WordCloud(width = 600, height = 300, background_color=\"white\",).generate(unique_string)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud_polluted\"+\".png\", bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBymy8rHokpp"
      },
      "source": [
        "## Quitar StopWords o Palabras Vacias\n",
        "\n",
        "Si vemos en el ejercicio anterior, hay muchas palabras con frecuencias altas que no nos dicen nada, estas son palabras vacias o Stopwords.\n",
        "\n",
        "Las palabras vac√≠as son aquellas palabras en lenguaje natural que tienen muy poco significado, como ¬´es¬ª, ¬´una¬ª, ¬´el¬ª, etc. \n",
        "\n",
        "Las palabras vac√≠as a menudo se eliminan del texto antes de entrenar los modelos de aprendizaje profundo y de Machine Learning, ya que las palabras vac√≠as ocurren en abundancia, por lo que brindan poca o ninguna informaci√≥n √∫nica que se pueda usar para la clasificaci√≥n o agrupaci√≥n.\n",
        "\n",
        "[Palabras Vacias Wikipedia](https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa)\n",
        "\n",
        "NLTK tiene un listado de palabras vacias en ingles y espa√±ol, pero tambien podemos a√±adir las nuestras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_gVC5DwjjyL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "otros_a_quitar = ['¬ª', ';', ',', '.','!','?','¬ø',',','rt', 'via' ]\n",
        " \n",
        "punctuation = list(string.punctuation)\n",
        "stop = stopwords.words('spanish') + stopwords.words('english') + punctuation\n",
        "print(stop)\n",
        "print('Total de stop words: ', len(stop))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYwqr-FlpQuV"
      },
      "outputs": [],
      "source": [
        "# Listas de frecuencias\n",
        "count_stop = Counter()\n",
        "count_hash = Counter()\n",
        "count_mentions = Counter()\n",
        "count_terms_only = Counter()\n",
        "\n",
        "# Listas de palabras\n",
        "lista_todas = []\n",
        "lista_hash = []\n",
        "lista_mentions = []\n",
        "lista_terms_only = []\n",
        "\n",
        "for tweet in df[\"Tweet_Text\"]:\n",
        "\n",
        "  # Crear una lista de todos los terminos que no est√°n en el stop\n",
        "  terms_stop = [term for term in tweet_tokenizer.tokenize(tweet) if term not in stop]\n",
        "  # Actualizar el contador\n",
        "  count_stop.update(terms_stop)\n",
        "  # Actualizar la lista\n",
        "  lista_todas.extend(terms_stop)\n",
        "\n",
        "  # Contar hashtags\n",
        "  terms_hash = [term for term in tweet_tokenizer.tokenize(tweet) if term.startswith('#')]\n",
        "  # Actualizar el contador\n",
        "  count_hash.update(terms_hash)\n",
        "  # Actualizar la lista\n",
        "  lista_hash.extend(terms_hash)\n",
        "\n",
        "  # Contar menciones\n",
        "  terms_mentions = [term for term in tweet_tokenizer.tokenize(tweet) if term.startswith('@')]\n",
        "  # Actualizar el contador\n",
        "  count_mentions.update(terms_mentions)\n",
        "  # Actualizar la lista\n",
        "  lista_mentions.extend(terms_mentions)\n",
        "\n",
        "  # Contar terms sin hash ni menciones\n",
        "  terms_only = [term for term in tweet_tokenizer.tokenize(tweet) if term not in stop and not term.startswith(('#', '@', 'http'))]\n",
        "  # Actualizar el contador\n",
        "  count_terms_only.update(terms_only)\n",
        "  # Actualizar la lista\n",
        "  lista_terms_only.extend(terms_only)\n",
        "\n",
        "print(count_stop.most_common(20))\n",
        "print(count_hash.most_common(20))\n",
        "print(count_terms_only.most_common(20))\n",
        "print(count_mentions.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NegNM9sWTScB"
      },
      "source": [
        "# WordCloud\n",
        "\n",
        "Una nube de palabras o nube de etiquetas es una representaci√≥n visual de las palabras que conforman un texto, en donde el tama√±o es mayor para las palabras que aparecen con m√°s frecuencia.\n",
        "\n",
        "Uno de sus usos principales es la visualizaci√≥n de las etiquetas de un sitio web, de modo que los temas m√°s frecuentes en el sitio se muestren con mayor prominencia. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxuXGnY0IVux"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "unique_string=(\" \").join(lista_terms_only)\n",
        "wordcloud = WordCloud(width = 600, height = 300, background_color=\"white\",).generate(unique_string)\n",
        "plt.figure(figsize=(30,16))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"wordcloud\"+\".png\", bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sJ0ZuNAUavi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45VhshWpUJk4"
      },
      "source": [
        "# WordCloud con Mascara\n",
        "\n",
        "Python nos permite componer un wordcloud usando una imagen como mascara, asegurate de buscar una foto con fondo blanco.\n",
        "**No olvides subir la imagen spiderman_mask.jpg junto al cuaderno.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fSLaSAXcRaF"
      },
      "outputs": [],
      "source": [
        "# Start with loading all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unique_string=(\" \").join(lista_terms_only)\n",
        "\n",
        "# Utilizar una imagen del internet como mascara\n",
        "#url_imagen_original = \"https://i.pinimg.com/originals/80/7b/29/807b29f756cc47621f92b3581e263272.jpg\"\n",
        "#mask = np.array(Image.open( requests.get(urlimagen_original, stream=True).raw ))\n",
        "\n",
        "# Abrir una imagen de mascara local\n",
        "mask = np.array(Image.open(\"spiderman_mask.jpg\") )\n",
        "\n",
        "wordcloud_por = WordCloud(background_color=\"white\", max_words=5000, mask=mask).generate(unique_string)\n",
        "\n",
        "# create coloring from image\n",
        "image_colors = ImageColorGenerator(mask)\n",
        "plt.figure(figsize=[20,20])\n",
        "plt.imshow(wordcloud_por.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n",
        "# store to file\n",
        "plt.savefig(\"spiderman_wordcloud.png\", format=\"png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c45tyPIDgPma"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbMNh7rjB5jH"
      },
      "source": [
        "# Exportar listas a Excel para an√°lisis posterior\n",
        "\n",
        "**Creamos una funci√≥n para exportar la lista de frecuencias de palabras a Excel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMDn3z_2_bTq"
      },
      "outputs": [],
      "source": [
        "def counter_to_excel(counter, filename):\n",
        "  palabras = []\n",
        "  for i in counter.items():\n",
        "    palabras.append([i[0],i[1]])\n",
        "  pd.DataFrame(palabras).to_excel(filename, header=False, index=False)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhQbE3phs0kC"
      },
      "source": [
        "Extraer la data a excel para procesarla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plvz2AwFozoj"
      },
      "outputs": [],
      "source": [
        "counter_to_excel(count_stop, \"count_stop.xlsx\")\n",
        "counter_to_excel(count_hash, \"count_hash.xlsx\")\n",
        "counter_to_excel(count_terms_only, \"count_terms_only.xlsx\")\n",
        "counter_to_excel(count_mentions, \"count_mentions.xlsx\")\n",
        "\n",
        "pd.DataFrame(lista_todas).to_excel('lista_todas.xlsx', header=False, index=False)\n",
        "pd.DataFrame(lista_hash).to_excel('lista_hash.xlsx', header=False, index=False)\n",
        "pd.DataFrame(lista_mentions).to_excel('lista_mentions.xlsx', header=False, index=False)\n",
        "pd.DataFrame(lista_terms_only).to_excel('lista_terms_only.xlsx', header=False, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An√°lisis de senimiento\n",
        "![alt text](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0cc39c70021d21fc0d2fd7a986ccd242bef86c29/6-Figure1-1.png)"
      ],
      "metadata": {
        "id": "0xpn3Iqx6h7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set(rc={'figure.figsize':(30,1)})\n",
        "\n",
        "def visualise_sentiments(data):\n",
        "  sns.heatmap(pd.DataFrame(data).set_index(\"Sentence\").T,center=0, annot=True, cmap = \"PiYG\")\n",
        "\n",
        "def visualise_sentiments_from_sentence(sentence):\n",
        "  visualise_sentiments({\n",
        "      \"Sentence\":[\"SENTENCE\"] + sentence.split(),\n",
        "      \"Sentiment\":[sid.polarity_scores(sentence)[\"compound\"]] + [sid.polarity_scores(word)[\"compound\"] for word in sentence.split()]\n",
        "  })"
      ],
      "metadata": {
        "id": "SH-ckdT56hfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gwBbBfnE65vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK Vader"
      ],
      "metadata": {
        "id": "ZO2PjBCb7ArK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "A7fgK22067Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i really like you but you can be so dumb some times\""
      ],
      "metadata": {
        "id": "YTF4Ji5Q7Peq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sid.polarity_scores(sentence)"
      ],
      "metadata": {
        "id": "qWRly8v47Eqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_sentiments_from_sentence(sentence)"
      ],
      "metadata": {
        "id": "sPB-AvtK7MES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Que pasa si le pasamos texto en espa√±ol\n",
        "visualise_sentiments_from_sentence(tweet)"
      ],
      "metadata": {
        "id": "9jdTDQp28KQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traducir\n",
        "Casi todos los m√≥delos de Analisis de sentimiento funcionan en Ingles, una soluci√≥n es traducir."
      ],
      "metadata": {
        "id": "UcqRtWeo728y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spanish-sentiment-analysis\n",
        "!pip install -q googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "hXnoe31s7VDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "from time import sleep\n",
        "\n",
        "def traducir(text, dest='en', **kwargs):\n",
        "    translator = Translator()\n",
        "    result = None\n",
        "    while result == None:\n",
        "        try:\n",
        "            result = translator.translate(text,**kwargs)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            translator = Translator()\n",
        "            sleep(0.5)\n",
        "            pass\n",
        "    return result.text.lower()"
      ],
      "metadata": {
        "id": "CrhBhQuV8G13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_tweet = traducir(tweet)\n",
        "print('ES', tweet)\n",
        "print('ES', english_tweet)"
      ],
      "metadata": {
        "id": "ORQeNAIz8c3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sid.polarity_scores(tweet)"
      ],
      "metadata": {
        "id": "hVya7WyB9cC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_sentiments_from_sentence(english_tweet)"
      ],
      "metadata": {
        "id": "OXvUplKw8fHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hagamos un ejemplo m√°s claro***"
      ],
      "metadata": {
        "id": "PiF-eNM1-yjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_negativo = \"@cnt son la peor operadora del mundo, ojal√° se quiebren pronto y dejen de estafar a las personas\""
      ],
      "metadata": {
        "id": "LDE-V0FI9ZBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sid.polarity_scores(tweet_negativo)"
      ],
      "metadata": {
        "id": "u11SHo3B-FMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_sentiments_from_sentence(tweet_negativo)"
      ],
      "metadata": {
        "id": "xxBMowUv-H-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_tweet = traducir(tweet_negativo)\n",
        "negative_tweet"
      ],
      "metadata": {
        "id": "a-nBN5NW-Nl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sid.polarity_scores(negative_tweet)"
      ],
      "metadata": {
        "id": "OgMZPBqf-XVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_sentiments_from_sentence(negative_tweet)"
      ],
      "metadata": {
        "id": "K1aIHK3B-ibc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small = df.sample(5)\n",
        "small"
      ],
      "metadata": {
        "id": "dT4yYVQ1-lJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet_traducido'] = df.Tweet_Text.apply(traducir)"
      ],
      "metadata": {
        "id": "eySJczhA_cD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BshIZhzIANdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tweet_traducido.apply(sid.polarity_scores)"
      ],
      "metadata": {
        "id": "15eymuuOCmRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una funci√≥n para obtener solo el valor compound de la polaridad y a√±adirlo a una columna del dataframe."
      ],
      "metadata": {
        "id": "ETDwMU2qJxSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_polarity_score_compound(text):\n",
        "    return sid.polarity_scores(text)['compound']"
      ],
      "metadata": {
        "id": "giLFZOHTEZFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentimiento'] = df.tweet_traducido.apply( get_polarity_score_compound )"
      ],
      "metadata": {
        "id": "bN1x9ccFFvBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sentimiento.hist()"
      ],
      "metadata": {
        "id": "OI_I53WiHD99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio"
      ],
      "metadata": {
        "id": "vVWSvpXqJ-AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. \n",
        "Importa un excel con un listado de Tweets en ingl√©s y almacenalos en un dataframe llamado df_tuits.\n",
        "Utiliza el cuaderno de [scrapping en twitter](https://colab.research.google.com/github/mayait/ClaseAnalisisDatos/blob/main/nlp/2022_Twitter_Data_Scrapper.ipynb)"
      ],
      "metadata": {
        "id": "sEn7LrE_KCMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_tuits = pd.read_excel..."
      ],
      "metadata": {
        "id": "bcQsD7-AKLog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.\n",
        "¬øC√∫ales son las palabras m√°s comunes de esa busqueda?\n",
        "Elabora una nube de palabras"
      ],
      "metadata": {
        "id": "EX_XadaUNj9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XC3Sq14yNttc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.\n",
        "- Utilizando tuits descargados en ingles\n",
        "- ¬øCuantos tuits son posittivos?\n",
        "- ¬øCuantos son negativos?\n",
        "- C√∫al es el sentimiento de la conversaci√≥n"
      ],
      "metadata": {
        "id": "_KSYMzw2NuwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mLsxn99DOGKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PjklGg_uOG6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tdNVZildOHSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Elabora un histograma del sentimiento de los tuits"
      ],
      "metadata": {
        "id": "9KLtzdvjOH05"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Tokenization_Sentiment_Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}